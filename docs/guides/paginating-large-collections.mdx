---
title: "Paginating Large Collections"
description: "how to work with collections that have thousands of records"
---

# how to paginate large collections

this guide shows you how to use cursor-based pagination to work with large collections efficiently.

## basic pagination workflow

### step 1: fetch the first page

```bash
pdsx -r jlowin.dev ls app.bsky.feed.post --limit 50
```

output:
```
app.bsky.feed.post (50 records)
3m53p7qp7e22v: {...}
3lyquoigrej2a: {...}
# ... 48 more records

next page cursor: 3lyqmkpiprs2w
```

### step 2: fetch the next page

copy the cursor and use it in the next request:

```bash
pdsx -r jlowin.dev ls app.bsky.feed.post --limit 50 --cursor 3lyqmkpiprs2w
```

### step 3: repeat until no cursor

when you've reached the end, no cursor will be displayed:

```
app.bsky.feed.post (23 records)
# ... records ...

# no cursor = end of collection
```

## processing all pages with bash

here's a script to fetch and process all pages:

```bash
#!/bin/bash

REPO="jlowin.dev"
COLLECTION="app.bsky.feed.post"
LIMIT=100
CURSOR=""

while true; do
    if [ -z "$CURSOR" ]; then
        # first page
        OUTPUT=$(pdsx -r "$REPO" ls "$COLLECTION" --limit "$LIMIT" -o json 2>&1)
    else
        # subsequent pages
        OUTPUT=$(pdsx -r "$REPO" ls "$COLLECTION" --limit "$LIMIT" --cursor "$CURSOR" -o json 2>&1)
    fi

    # extract and process records (stdout)
    echo "$OUTPUT" | grep -v "next page cursor" | jq -r '.[] | .text'

    # extract next cursor from stderr
    CURSOR=$(echo "$OUTPUT" | grep "next page cursor" | awk '{print $NF}')

    # if no cursor, we're done
    [ -z "$CURSOR" ] && break

    # be nice to the server
    sleep 1
done
```

save as `fetch_all.sh`, make executable:
```bash
chmod +x fetch_all.sh
./fetch_all.sh
```

## processing all pages with python

```python
import subprocess
import json
from typing import List, Dict, Any

def fetch_all_pages(
    repo: str,
    collection: str,
    limit: int = 100
) -> List[Dict[str, Any]]:
    """fetch all pages of a collection."""
    all_records = []
    cursor = None

    while True:
        # build command
        cmd = [
            "pdsx",
            "-r", repo,
            "ls", collection,
            "--limit", str(limit),
            "-o", "json"
        ]
        if cursor:
            cmd.extend(["--cursor", cursor])

        # execute
        result = subprocess.run(cmd, capture_output=True, text=True)

        # parse records from stdout
        records = json.loads(result.stdout)
        all_records.extend(records)

        # check stderr for next cursor
        if "next page cursor:" in result.stderr:
            cursor = result.stderr.split("next page cursor:")[-1].strip()
        else:
            # no more pages
            break

    return all_records

# usage
posts = fetch_all_pages("jlowin.dev", "app.bsky.feed.post")
print(f"fetched {len(posts)} total posts")

# process records
for post in posts:
    print(f"- {post.get('text', '')[:50]}...")
```

## exporting to file

### export as jsonl (one record per line)

```bash
#!/bin/bash
REPO="jlowin.dev"
COLLECTION="app.bsky.feed.post"
OUTPUT_FILE="posts.jsonl"
CURSOR=""

> "$OUTPUT_FILE"  # clear file

while true; do
    if [ -z "$CURSOR" ]; then
        OUTPUT=$(pdsx -r "$REPO" ls "$COLLECTION" --limit 100 -o json 2>&1)
    else
        OUTPUT=$(pdsx -r "$REPO" ls "$COLLECTION" --limit 100 --cursor "$CURSOR" -o json 2>&1)
    fi

    # append each record as a line
    echo "$OUTPUT" | grep -v "next page cursor" | jq -c '.[]' >> "$OUTPUT_FILE"

    CURSOR=$(echo "$OUTPUT" | grep "next page cursor" | awk '{print $NF}')
    [ -z "$CURSOR" ] && break
    sleep 1
done

echo "exported to $OUTPUT_FILE"
```

### export as csv

```bash
#!/bin/bash
REPO="jlowin.dev"
COLLECTION="app.bsky.feed.post"
OUTPUT_FILE="posts.csv"
CURSOR=""

# write header
echo "rkey,created_at,text" > "$OUTPUT_FILE"

while true; do
    if [ -z "$CURSOR" ]; then
        OUTPUT=$(pdsx -r "$REPO" ls "$COLLECTION" --limit 100 -o json 2>&1)
    else
        OUTPUT=$(pdsx -r "$REPO" ls "$COLLECTION" --limit 100 --cursor "$CURSOR" -o json 2>&1)
    fi

    # convert to csv rows
    echo "$OUTPUT" | grep -v "next page cursor" | \
        jq -r '.[] | [.rkey, .created_at, .text] | @csv' >> "$OUTPUT_FILE"

    CURSOR=$(echo "$OUTPUT" | grep "next page cursor" | awk '{print $NF}')
    [ -z "$CURSOR" ] && break
    sleep 1
done

echo "exported to $OUTPUT_FILE"
```

## filtering while paginating

filter records as you paginate to save memory:

```python
import subprocess
import json
from datetime import datetime

def fetch_posts_since(repo: str, since: str) -> List[Dict]:
    """fetch posts created after a specific date."""
    posts = []
    cursor = None
    since_dt = datetime.fromisoformat(since.replace('Z', '+00:00'))

    while True:
        cmd = ["pdsx", "-r", repo, "ls", "app.bsky.feed.post", "--limit", "100", "-o", "json"]
        if cursor:
            cmd.extend(["--cursor", cursor])

        result = subprocess.run(cmd, capture_output=True, text=True)
        records = json.loads(result.stdout)

        for record in records:
            created_at = datetime.fromisoformat(record['created_at'].replace('Z', '+00:00'))
            if created_at > since_dt:
                posts.append(record)

        if "next page cursor:" in result.stderr:
            cursor = result.stderr.split("next page cursor:")[-1].strip()
        else:
            break

    return posts

# get posts from last week
recent = fetch_posts_since("jlowin.dev", "2025-11-01T00:00:00Z")
print(f"found {len(recent)} recent posts")
```

## rate limiting considerations

when paginating through large collections, be mindful of rate limits:

```python
import time

def fetch_all_with_backoff(repo: str, collection: str):
    """fetch all pages with rate limit handling."""
    all_records = []
    cursor = None
    page_count = 0

    while True:
        cmd = ["pdsx", "-r", repo, "ls", collection, "--limit", "100", "-o", "json"]
        if cursor:
            cmd.extend(["--cursor", cursor])

        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode != 0:
            # if rate limited, wait and retry
            if "429" in result.stderr or "rate limit" in result.stderr.lower():
                print("rate limited, waiting 60s...")
                time.sleep(60)
                continue
            else:
                raise Exception(f"error: {result.stderr}")

        records = json.loads(result.stdout)
        all_records.extend(records)
        page_count += 1

        if "next page cursor:" in result.stderr:
            cursor = result.stderr.split("next page cursor:")[-1].strip()
            # wait between pages to avoid rate limits
            time.sleep(1)
        else:
            break

    print(f"fetched {len(all_records)} records across {page_count} pages")
    return all_records
```

## performance tips

### choose appropriate limit

- small limits (10-25): good for testing, slower overall
- medium limits (50): balanced
- large limits (100): fewer requests, faster total time

```bash
# faster: fewer requests
pdsx ls collection --limit 100

# slower: more requests
pdsx ls collection --limit 10
```

### parallel processing

process pages in parallel (if order doesn't matter):

```python
from concurrent.futures import ThreadPoolExecutor
import subprocess
import json

def fetch_page(repo: str, collection: str, cursor: str = None):
    """fetch a single page."""
    cmd = ["pdsx", "-r", repo, "ls", collection, "--limit", "100", "-o", "json"]
    if cursor:
        cmd.extend(["--cursor", cursor])

    result = subprocess.run(cmd, capture_output=True, text=True)
    records = json.loads(result.stdout)

    next_cursor = None
    if "next page cursor:" in result.stderr:
        next_cursor = result.stderr.split("next page cursor:")[-1].strip()

    return records, next_cursor

def process_records(records):
    """process a batch of records."""
    for record in records:
        # do something with record
        print(f"processing {record['rkey']}")

# fetch first few pages to get cursors
cursors = [None]  # start with no cursor
for _ in range(10):  # get 10 cursors
    records, next_cursor = fetch_page("repo.handle", "app.bsky.feed.post", cursors[-1])
    if next_cursor:
        cursors.append(next_cursor)
    else:
        break

# process pages in parallel
with ThreadPoolExecutor(max_workers=5) as executor:
    futures = []
    for cursor in cursors:
        records, _ = fetch_page("repo.handle", "app.bsky.feed.post", cursor)
        future = executor.submit(process_records, records)
        futures.append(future)

    # wait for all to complete
    for future in futures:
        future.result()
```

## common issues

### cursor expired
if you wait too long between requests:

```
error: cursor expired or invalid
```

**solution**: restart from beginning, use cursors immediately

### inconsistent parameters
don't change limit between pages:

```bash
# wrong
pdsx ls collection --limit 50
pdsx ls collection --limit 100 --cursor abc123  # different limit!

# correct
pdsx ls collection --limit 50
pdsx ls collection --limit 50 --cursor abc123  # same limit
```

### json parsing errors
if cursor appears in json output:

```bash
# wrong - old pdsx version
pdsx ls collection -o json | jq  # fails

# correct - new pdsx version
pdsx ls collection -o json | jq  # cursor goes to stderr
```

## next steps

- learn about [cursor-based pagination concepts](../concepts/pagination.mdx)
- see [working with records](./working-with-records.mdx)
- check [quickstart](../quickstart.mdx) for basics
